%!TEX root = ../main.tex

\chapter{The background after the LAr veto cut}%
\label{chap:bkg:lar:ph2}

All what has been shown until now concerns data before the LAr and PSD cut. In this
chapter, instead, a model of the background after the LAr veto cut will be presented,
based on a Monte Carlo simulation of the LAr scintillation light propagation. Being able
to describe the background after this major event selection is indeed of great interest to
study the distribution of two-neutrino double-beta decay events, which are almost never
vetoed by the LAr veto system\footnote{%
  Double-beta decay interactions in germanium release energy in about 1~mm from the
  interaction vertex, the probability of the emitted electrons to escape the detector and
  interact with LAr is very small.
}. As extensively shown in \cref{chap:theory}, the presence of several new physics
phenomena can be constrained by studying the shape of the \nnbb\ events distribution.
Understanding the action of the LAr veto cut on background events from the point of view
of the background model requires, however, a full Monte Carlo simulation of the LAr
scintillation mechanism as well as a complete implementation of the relevant material and
surface optical properties that contribute to light propagation in the setup.
Implementing such a simulation, as it will be clear soon, requires an accurate knowledge
of many optical parameters, which is, unfortunately, not always available.
Nevertheless, it will be shown how special calibration data with low-activity sources and
the LAr veto instrumentation turned on can be used to partially overcome the issue. An
independent analysis of this special data set is used to tune the unknown optical
parameters in the Monte Carlo and reproduce the observed vetoing performance.  The
obtained parameters are then used to generate a map of the LAr scintillation light
detection probability, which is applied to the background model simulations in order to
obtain the LAr veto flag. These new background model \pdf{}s are then directly employed in a
statistical test of possible deviations of the \nnbb\ distribution from its Standard Model
description, an analysis that will be presented in \cref{chap:2nbb-ana}.
\newpar
The chapter is structured as follows: in \cref{sec:bkg:lar:ph2:data} the data after the
LAr veto cut, sub-divided in the analysis data sets, is described. Follows a description
of the LAr veto system simulation, embedded into the \mage\ framework, in
\cref{sec:bkg:lar:ph2:pdfs}. There it will be also shown how special calibration data is
used to tune the simulation model and how the LAr veto flag for synthetic events is
evaluated. Finally, the background decomposition of the full-range \gerda\ energy spectrum
after the LAr veto cut (\gexpophasetwobkg\ from the first part of \phasetwo) will be
presented in \cref{sec:bkg:lar:ph2:gmodel} and discussed in
\cref{sec:bkg:lar:ph2:discussion}.

\section{Analysis data sets}%
\label{sec:bkg:lar:ph2:data}

\begin{table}
  \centering
  \caption{%
    Properties of the data sets considered in this analysis. Further details about the
    \gerda\ detectors can be found in past publications~\cite{Agostini2013a,
    Agostini2018a}. Note that the exposures are slightly higher than those reported in
    \cref{tab:bkg:raw:ph2:datasets} because of additional data from the last run of the
    first part of \phasetwo\ (\run{93}) which was not included in the first place.
  }\label{tab:bkg:lar:ph2:datasets}
  \small
  \input{tab/bkg/lar/ph2/datasets.tex}
\end{table}%
\footnotetext{%
  The \bege\ detector \GD{02D} is the only detector that does not fully
  deplete~\cite{Agostini2018a}. Hence, events triggered by this detector are
  not considered in either data set and it is omitted from the mass
  computation.
}

The background model after the LAr veto cut has been developed using data from the first
part of \gerdatwo, as in \cref{chap:bkg:raw:ph2}. Single- and two- detector events that
survive the LAr veto cut (more in \cref{sec:gerda:cuts}) have been considered: two data
sets from the first category (\enrBEGeII\ and \enrCoaxII) and a single one for the second
(\enrGeII). The data set exposures are documented in \cref{tab:bkg:lar:ph2:datasets}.
Note that the exposures are slightly higher than those reported in
\cref{tab:bkg:raw:ph2:datasets} because of additional data from the last run of the first
part of \phasetwo\ (\run{93}) which was not included in the first place.  The reader is
referred to \cref{sec:bkg:raw:ph2:data} for general details about how these data sets are
constructed.
\newpar
The event energy distributions of the three data sets before and after the LAr veto cut
are displayed in \cref{fig:bkg:lar:ph2:data-desc}: the sum spectrum of \enrBEGeII\ and
\enrCoaxII\ in the top panel and \enrGeII\ in the bottom panel. \g\ peaks and their
Compton shoulders are largely suppressed (e.g.~80\% of the \kvz\ FEP is cut) with the
exception of \kvn, which is a pure \g\ emission (electron capture) and is less likely to
deposit coincident energy in the LAr. After the cut, the spectrum is mainly composed by
\nnbb\ (the LAr veto survival probability is $>99$\%) and residual \a\ events. The
signal-to-background ratio in the \nnbb-dominated region (from 565~keV to 2000~keV
excluding the potassium \g\ lines) improves by a factor 10, jumping from $\sim$2 to
$\sim$~20. This large background reduction motivates the development of a background model
after the LAr veto cut to study the \nnbb\ event distribution with a significant
sensitivity improvement.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/data-desc.pdf}
  \caption{%
    The data from the first \gexpophasetwobkg\ of \gerdatwo\ before and after the liquid argon veto
    cut, divided into the three background model data sets: \enrBEGeII, \enrCoaxII\ and
    \enrGeII.
  }\label{fig:bkg:lar:ph2:data-desc}
\end{figure}

\section{Monte Carlo simulations and probability density functions}%
\label{sec:bkg:lar:ph2:pdfs}

The Monte Carlo simulation of the liquid argon veto requires to enable the \geant\ optical
physics simulation routines, which have been disabled for the production of the background
model \pdf{}s before analysis cuts (see \cref{sec:bkg:raw:ph2:pdfs}). The relevant material
optical properties which are needed to simulate the scintillation of LAr and the
propagation of the photons throughout the whole setup have been implemented into \mage\
and documented in detail in \cref{sec:apdx:mage-optics}. As reported there, unfortunately,
many properties are uncertain or not known at all, especially in the VUV energy regime
($\sim$128~nm).  This uncertainty propagates to the LAr veto model and the background
\pdf{}s, and is treated as a systematic contribution in the \nnbb\ distribution analysis
presented in \cref{chap:2nbb-ana}.

\subsection{Simulating the LAr veto system}%
\label{sec:bkg:lar:ph2:heatmap}

Simulating optical physics is notoriously a computationally intensive task, as the number
of photons that need to be tracked is very high. Enabling optical processes in background
model simulations, which already take tens of thousands of CPU hours to complete, is not
feasible. To address the problem, an alternative approach to compute the LAr veto flag for
already existing simulations has been developed, based on the construction of a detection
probability map of scintillation photons in LAr. This object, which is going to be
described in this section, will be also be referred to just as ``heat map'' or
``probability map'' in the following.
\newpar
The first step to produce the probability map is to run a full photon-tracking simulation
of 128~nm scintillation photons in the whole LAr volume defined in \mage. An isotropic
source of VUV photons, homogeneously distributed in a pre-selected LAr volume, is
simulated.  \mage, in fact, allows to restrict the sampling to the volume occupied by LAr
only or its intersection with a geometric solid (e.g.~a cylinder). The photon initial
energy is sampled from a Gaussian distribution with mean 128~nm and variance
2.929~nm~\cite{Heindl2010}. After being propagated in the \gerda\ setup by the \geant\
core routines, it may hit a LAr instrumentation sensitive volume (SiPM channel or PMT
channel), whose unique identification number is written on disk. After collecting
a sufficient amount of simulated events, the simulation output is further processed into
the probability map. The three-dimensional LAr volume implemented in \mage\ is partitioned
in small boxes (or voxels), that define the regions in which the probability is constant.
Events generated in a voxel are collected and the ratio between the number of detected
photons (in which at least one LAr veto channel fired) over the total is computed:
\[
  p_k = \frac{n_k}{N_k} \pm \frac{1}{N_k}\sqrt{n_k \left(1 - \frac{n_k}{N_k} \right)} \;.
\]
where $n_k$ and $N_k$ are the total number of detected and simulated photons in voxel $k$,
respectively. The binomial uncertainty estimate assumes $N_k>0$ and $n_k<N_k$, which is
always the case for the voxel size considered in this study.  These probability estimates
are written on disk as a three-dimensional histogram, or probability map. The \gerda\
Monte Carlo LAr model is effectively condensed in this object.
\newpar
The second and last step is to fold the probability map into the usual background model
simulation output, for which no information derived from native optical processes is
available.  Nevertheless, information about energy depositions by \g, \b\ and \a\
particles in LAr is available in the simulation output and provide the starting point to
compute the LAr veto flag. For a given single energy deposition the number of generated
scintillation photons $M$ is drawn from a Poisson distribution with mean equal to the
deposited amount of energy times the LAr scintillation yield times the LAr Fano factor:
\[
  M \sim \operatorname{P}(E \cdot Y \cdot F) \;.
\]
The number of detected photons is then randomly drawn from a binomial distribution with
success probability $p_k$, where $k$ labels the voxel that contains the LAr hit position,
and number of trials equal to $M$:
\[
  m \sim \operatorname{B}(M, p_k) \;.
\]
If $m>0$ the event is flagged as vetoed by the LAr instrumentation.

\blocktitle{the LAr \\ heat map}
\sloppy A visualization of an example probability map is given in
\cref{fig:bkg:lar:ph2:larmap:tac}, along with one-voxel wide transversal and longitudinal
slices. Voxels are colored according to their probability value: darker areas correspond
to regions in which less scintillation photons reach the LAr veto detectors and therefore
the vetoing efficiency is worse. The detection probability reaches very low values in the
LAr volume enclosed by germanium detectors, as photons get easily trapped in such a
complex geometry. On the other hand, the closer to PMTs and fibers the photons are
produced, the higher their detection probability is. In the horizontal slice one can also
appreciate the effect of the different SiPM channel efficiencies, which break the
rotational invariance of the map.
\newpar
\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{plots/bkg/lar/ph2/larmodel/larmap-tac.png}
  \caption{%
    The three-dimensional LAr photon detection probability map interactive viewer.
    Two-dimensional longitudinal and transversal sections are displayed in the second and
    third pad from the left corresponding to the user pointer position on the 3D
    rendering in the first pad. Red lines mark the cut positions. A smoothing algorithm is
    applied to wash out statistical fluctuations and make the map look more homogeneous to
    the eye.
  }\label{fig:bkg:lar:ph2:larmap:tac}
\end{figure}
As remarked in \cref{sec:apdx:mage-optics}, uncertainties on the optical specifications
implemented in \mage\ can be quite large. Properties like the LAr scintillation yield and
attenuation length, the germanium reflectivity and the TPB quantum efficiency have a
potentially large impact on the probability map. Other crucial unknowns are the SiPM and PMT
channel efficiencies and the coverage of the fiber shroud, defined as the fraction of
lateral surface area of the curtain occupied by fiber material. Channel efficiencies
extracted from physics data cannot be used, as the simulated efficiencies account for
various other effects in the Monte Carlo and can therefore be quite
different\footnote{\label{footnote:bkg:lar:ph2:mc-eff}%
  Suppose, as an example, that a systematic difference between the geometry of an
  implemented setup part and the experimental reality is present. Suppose also that this
  impacts systematically on the light seen by sensitive detectors in the Monte Carlo. A
  very well possible scenario might be that, for example, the trapping efficiency of a
  fiber module in the simulation largely deviates from reality. In this sense, the
  efficiency attached to a SiPM or PMT channel in the Monte Carlo in order to reproduce
  the event suppression seen in data is \emph{effective}, i.e.~must include several
  additional effects arising from an approximate modeling.
}. The fiber coverage on the other hand should be around 0.5, but there could be shrinking
phenomena or single-fiber twists in LAr which could make the real coverage significantly
different.
\newpar
To understand the systematic impact of these parameters on the LAr probability map, a
dedicated Monte Carlo study has been performed. A set of representative voxels has been
selected, whose location is documented in \cref{fig:bkg:lar:ph2:larmap:dist}, top panel.
For each of these voxels the probability dependence on some Monte Carlo parameters has
been investigated with dedicated full photon-tracking simulations. The results are
displayed in the remaining plots of \cref{fig:bkg:lar:ph2:larmap:dist}. Voxels have been
considered along the central array axis (green), just outside (blue) and inside (red) the
fiber shroud. Three additional voxels have been chosen in the low-probability region
inside the germanium array (black and violet). Four properties have been taken into
consideration for this study: the germanium reflectivity, the fiber shroud coverage,
the LAr absorption length and the quantum efficiency of TPB-coated surfaces. Each
probability curve is normalized to the value corresponding to reference parameters, such
that the vertical axis shows relative changes. Two qualitatively different trends can be
noticed: the reflectivity and coverage impact is approximately linear in the considered
interval, while the absorption length acts more exponentially on probabilities.  This is
compatible with the assumption that attenuation in matter generally follows an exponential
law. The impact of a parameter depends on the voxel location too. As instance, the
probability in the black voxel between \GD{89B} and \GD{02D} changes drastically upon
different germanium reflectivity assumptions. On the other hand, the orange and red voxel
close to the fibers (where the calibration sources are) are the most sensitive to
modifications of the fiber shroud coverage. Acting on the TPB quantum efficiency seems to
uniformly scale the detection probability, as one would do by multiplying the heat map by
a global factor.

\begin{figure}
  \centering
  \includegraphics[height=0.8\textwidth, angle=90]{plots/bkg/lar/ph2/larmodel/lar-points-position.pdf}
  \includegraphics{plots/bkg/lar/ph2/larmodel/larmap-dist.pdf}
  \caption{%
    Study of the impact of Monte Carlo parameters on LAr light detection probabilities in
    various spatial points. Each curve shows the dependence of the probability (normalized
    to the values obtained with the parameter reference value) on germanium reflectivity,
    fiber shroud coverage and LAr absorption length in the points shown in the top panel,
    using the same color code. The germanium reflectivity, which is the only quantity that
    depends on the incident photon energy, is scaled by a global factor, shown on the
    horizontal axis. A unit value of the scaling factor corresponds to the reference
    values implemented in \mage.
  }\label{fig:bkg:lar:ph2:larmap:dist}
\end{figure}

\subsection{Tuning the LAr veto Monte Carlo model}%
\label{sec:bkg:lar:ph2:pcalib}

As mentioned in the previous section, the knowledge of several Monte Carlo optical
specifications is unfortunately poor. In particular, PMT and SiPM channel efficiencies,
essential to build a predictive LAr veto model, are not known. Efficiencies extracted from
physics data cannot be used directly, as the Monte Carlo efficiencies are in reality
complex objects that account for other
effects\textsuperscript{\ref{footnote:bkg:lar:ph2:mc-eff}}. To overcome these issues, a
statistical analysis has been developed to tune the Monte Carlo parameters with physics
data. A sample which is independent from the regular \gerda\ physics data has been
identified in the special calibration runs with low-activity sources and the LAr veto
instrumentation turned on. A short overview of this analysis will be given in this
section; the reader is referred to~\cite{Wiesinger2021} for an extensive presentation of
this broad subject.

\blocktitle{pcalib \\ runs}
The main characteristics of these data sets are documented in
\cref{tab:bkg:lar:ph2:pcalib-desc}: the first special run carries identification number
\oldstylenums{68} and has been performed with a \Th\ source in July \yr{2016}, while the second one
is \run{76} and has been carried through in February \yr{2017} with a \Ra\ source. Data has been
acquired with sources \m{S2} and \m{S3} at three vertical positions at the top, middle and
bottom of the array (see\cref{tab:bkg:lar:ph2:pcalib-desc}, right). The lower source
activity ($\mathcal{O}$(kBq)) makes it possible to collect data with the LAr veto
instrumentation switched on and provide a distinct setting for an accurate
data-to-Monte-Carlo comparison. Since the purpose of this special data taking is to study
the LAr vetoing performance, the germanium main trigger was maintained during the
data taking. Test pulses are also available and are used to estimate the fraction of
\emph{random coincidences}. In these particular, false-positive, LAr-vetoed events the
physical process generating the coincident scintillation light detected by PMTs and SiPMs
is distinct from the one that triggers the germanium detectors. This can happen, for
example, if the decay products of a nucleus in the calibration source deposits energy in
the germanium while a cosmic ray is ionizing the argon. Similarly, random coincidences can
be produced by two nuclei decaying at the same time: one of them triggers the germanium
and the other one produces coincident light. A good estimate of the fraction of random
coincidences seen in data is crucial when comparing to simulations, in which this
background is missing.

\begin{table}
  \centering
  \caption{%
    Summary of the special calibration data with active LAr veto instrumentation from
    \run{68} (July \yr{2016}, \Th\ source) and \run{76} (February \yr{2017}, \Ra\ source). The
    random coincidences are estimated combining data from test pulses and SiPM
    traces~\cite{Wiesinger2021}. Unfortunately, pulser data is partly missing in \run{68}.
    On the right: a visualization of the source position at three different heights.
  }\label{tab:bkg:lar:ph2:pcalib-desc}
  \footnotesize
  \begin{tabular}{lcccc}
    \toprule
    isotope      & source port & position (mm) & run time (h) & random coincidences (\%) \\
    \midrule
    \mr{6}{\Th}  &             & 8168          & 10.2         & --                       \\
                 & \m{C2}      & 8396          & 3.2          & --                       \\
                 &             & 8570          & 12.5         & --                       \\
                 \cmidrule{2-5}
                 &             & 8220          & 6.4          & $7.5 \pm 0.6$            \\
                 & \m{C3}      & 8405          & 4.3          & $7.2 \pm 1.0$            \\
                 &             & 8570          & 3.6          & $10.2 \pm 1.4$           \\
    \midrule
    \mr{6}{\Ra}  &             & 8139          & 8.9          & $12.2 \pm 0.3$           \\
                 & \m{C2}      & 8405          & 4.3          & $11.2 \pm 0.4$           \\
                 &             & 8570          & 6.9          & $12.9 \pm 0.3$           \\
                 \cmidrule{2-5}
                 &             & 8128          & 8.0          & $10.8 \pm 0.3$           \\
                 & \m{C3}      & 8292          & 3.6          & $8.9 \pm 0.4$            \\
                 &             & 8570          & 8.5          & $10.7 \pm 0.3$           \\
    \bottomrule
  \end{tabular}%
  \hspace{0.5cm}%
  \raisebox{-3.6cm}{%
    \begin{tikzpicture}
      \node[anchor=west] at (0,-0.4) {\includegraphics[height=6.5cm,clip,trim=270 0 0 0]{gedet/phII-array-calib-side.png}};
      \draw[thick, dash pattern={on 7pt off 2pt on 1pt off 3pt}] (0.1,-3.2) -- (0.1,3.2);
    \end{tikzpicture}
  }
\end{table}

\blocktitle{data \\ selection}
The analysis data set is constructed by applying event cuts based on the total energy
released in the germanium detectors. The selected energy regions are shown in
\cref{fig:bkg:lar:ph2:pcalib-data}: the \Tl\ full-energy peak events ($2615 \pm 10$~keV)
in \run{68} and the Compton-dominated energy region from the 2204~keV \Bih\ line in
\run{76} are considered. The obtained statistics is about $2 \cdot 10^4$ events or more
per source position.  The data from the LAr instrumentation which is relevant for the
comparison with Monte Carlo simulations consists in a set of veto flags (one for each of
the 25 LAr veto channels, i.e.~9 top PMTs, 7 bottom PMTs and 9 SiPM modules) for each
germanium trigger.  The probability that a LAr veto channel is triggered in data can be
written as the convolution between a \emph{signal} probability (i.e.~the same event is
responsible for both the germanium and LAr veto triggers) and a \emph{background}
probability (i.e.~the false-positive rate from random coincidences). This convolution is
simply the logic OR between the two probabilities. The amount of random coincidences is
estimated by combining data from test pulses and SiPM traces, as documented in great
detail in~\cite{Wiesinger2021}.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{plots/bkg/lar/ph2/larmodel/pca-analysis-window.pdf}
  \caption{%
    The energy spectra of the \phasetwo\ special calibration runs. Left: \run{68} (\Th),
    Monte Carlo simulation in blue and data in black. The top panel shows the energy
    distribution of the events while the bottom panel shows the average amount of
    energy released in LAr by an event with respect to its energy. Right: \run{76} (\Ra).
    Colored bands highlight the regions selected for the data analysis.
  }\label{fig:bkg:lar:ph2:pcalib-data}
\end{figure}

\blocktitle{simulations}
The calibration sources are fully implemented in \mage, with user commands to set their
vertical position, the radioactive source and therefore replicate \run{68} and \run{76}
experimental settings (\cref{tab:bkg:lar:ph2:pcalib-desc}). Optical
processes are enabled in these special simulation runs, but, since they require high
computational time, photons are fully tracked only if an energy deposition is recorded in
germanium as well. The optical properties of the setup are fixed to their best values
documented in \cref{sec:apdx:mage-optics}. The two most important Monte Carlo settings
for the calibration source physics are the LAr attenuation length and the fiber shroud
coverage. As demonstrated in \cref{sec:bkg:lar:ph2:heatmap} and in
\cref{fig:bkg:lar:ph2:larmap:dist} in particular, these two parameters induce the largest
deviations of the LAr light detection probability in the red and orange points~---~where the
calibration sources are typically deployed (see \cref{tab:bkg:lar:ph2:pcalib-desc}). The
germanium reflectivity, instead, is crucial when probing the array region. Changes in the
LAr light yield or the TPB quantum efficiencies produce almost linear and homogeneous
distortions of the probability map and can be absorbed in the PMT and SiPM channel
efficiencies.

\blocktitle{statistical \\ analysis}
The probability to detect $n$ LAr scintillation photons with the LAr veto instrumentation
includes a signal component (the light is physically correlated to the germanium signal)
and a background component from random coincidences:
\[
  \lambda[n] = \lambda_s[n] * \lambda_b[n] = \lambda_s \vee \lambda_b \;.
\]
A way to introduce an effective detection efficiency $\epsilon$ for a LAr veto channel is
by the following ``binomial repopulation'':
\[
  \lambda_s[m](\epsilon) = \sum_{n \geq m} \lambda_s[n] \binom{n}{m} \epsilon^m
    {(1-\epsilon)}^{n-m} \;,
\]
which is the probability, reduced by the efficiency $\epsilon \in [0,1]$, to observe $m <
n$ photons. Since the quantity of interest for this analysis is the LAr veto flag (i.e.~an
event is seen by the instrumentation or not), the probability to detect $m > 0$ photons
can be expressed as
\[
  \lambda_s(\epsilon) = 1 - \lambda[0](\epsilon)
                      = 1 - \sum_n \lambda_s[n] {(1-\epsilon)}^n
                      = 1 - \frac{1}{N_\text{tot}} \sum_n N_n {(1-\epsilon)}^n \;,
\]
where the last equality holds for the Monte Carlo simulations, in which $\lambda_s[n]$ is
just the ratio between the number of events in which $n$ photons were detected and the
total number of events: $\lambda_s[n] = N_n / N_\text{tot}$.
\newpar
A likelihood function is then constructed to match the Monte Carlo simulation output to the
special calibration data. Since each single event includes data from 25 LAr veto channels,
also the particular detection \emph{pattern} must be taken into account. Given three channels
$A$, $B$ and $C$, for example, the pattern $\{A,C\}$ represents the occurence of a signal
in channel $A$, $C$ but not in $B$. The pattern in the example has probability $p_A
\cdot (1-p_B) \cdot p_C$, i.e.~it always probes all the channel detection probabilities
$p_i$. The full likelihood reads
\begin{equation}\label{eq:bkg:lar:ph2:pca-likelihood}
  \mathcal{L}(\vec{\epsilon}, \ldots) =
    \prod_P \operatorname{B}_{N_\text{tot}}^N \Big(
      \sum_G \big[ \lambda_s(\vec{\epsilon}) + \sigma \cdot
      \Delta\lambda_s(\vec{\epsilon})\big] \cdot \lambda_b 
    \Big)
    \cdot \prod_G \operatorname{B}_{M_\text{tot}}^M (\lambda_b) \cdot
    \operatorname{G}(\sigma) \;,
\end{equation}
where the first product runs over all possible $\dim{[P]} = 2^{N_\text{ch}}$ patterns
($N_\text{ch}$ is the number of considered channels), the summation and the last product
run over all the pattern generator pairs $G = \{\{A,B\},\{B,C\},\ldots\}$. In the first
binomial term $\operatorname{B}^N_{N_\text{tot}}(\lambda)$, $N$ is the number of events in
which light was seen over the total $N_\text{tot}$ for a certain pattern. The same
nomenclature applies for the second binomial term
$\operatorname{B}^M_{M_\text{tot}}(\lambda)$, but for the random coincidence data set.
The probability in the first binomial term is the product between the signal probability
for a given pattern (which is calculated by summing over all possible pattern generators)
and the background random coincidence probability $\lambda_b$. The signal probability
contains an additional contribution $\sigma \Delta\lambda_s(\vec{\epsilon})$ which
accounts for the effect of low statistics in the Monte Carlo data sample. The term is
regulated by the nuisance parameter $\sigma$, which is constrained by the pull term
$\operatorname{G}(\sigma)$, a Gaussian distribution with null mean and variance $\sigma$. The
number of degrees of freedom in this likelihood is $2^{N_\text{ch}}-N$.
\newpar
The likelihood function is then maximized to obtain the best fit values for the parameters
of interest, namely the LAr veto channel efficiencies $\vec{\epsilon}$. The same likelihood
can also be used to make some inference on other Monte Carlo optical unknowns,
e.g.~the LAr attenuation length or the fiber shroud coverage. This idea is presented and
discussed in~\cite{Wiesinger2021}, but its application is out of the scope of this work,
which requires only a rough tuning of the LAr probability map. Indeed, a broad range of
systematic map distortions which might be due to uncertain optical specifications is
tested in the framework of the \nnbb\ distribution analysis, which will be presented in
\cref{chap:2nbb-ana}.

\blocktitle{results}
Since the tuned LAr probability map will be only used to provide the LAr veto flag for the
background model simulations, which are further processed to create the \pdf{}s for the
\bege\ summed energy spectrum (see \cref{chap:2nbb-ana}), an additional simplification can be
introduced in the analysis. In principle, the array of channel efficiencies
$\vec{\epsilon}$ in \cref{eq:bkg:lar:ph2:pca-likelihood} has dimension 25, but the
symmetries of the experimental setup can be exploited to reduce the number of parameters.
A cylindrical symmetry is \emph{de facto} present in the arrangement of PMTs and SiPM
modules, as shown in the technical drawings in \cref{fig:setup:magevolumes}. This spatial
symmetry is evidently broken in the LAr veto channel efficiencies domain, since the
measured signal rates are already different one from the other, but the effect has to be
evaluated on the analysis data set, i.e.~the \bege\ summed energy spectrum. Since the
\bege\ detector arrangement in the array does not significantly deviate from a cylindrical
distribution, it is not expected to depend too much on differences between efficiencies of
light detectors at the same vertical height. Therefore, only three effective efficiencies
are used in \cref{eq:bkg:lar:ph2:pca-likelihood} ($N_\text{ch}=3$): one for all top PMTs,
one for all SiPM modules and one for all bottom PMTs.  In this simplified setting, the
maximization of $\mathcal{L}(\vec{\epsilon})$ yields:
\begin{equation}\label{eq:bkg:lar:ph2:chan-eff}
  \epsilon_\text{PMTt} = 0.140 \pm 0.003 \;, \quad
  \epsilon_\text{SiPM} = 0.326 \pm 0.007 \;, \quad
  \epsilon_\text{PMTb} = 0.346 \pm 0.007 \;.
\end{equation}
The magnitude of systematic uncertainty needed to obtain a reasonable goodness-of-fit
(\pvalue~$\sim 0.3$) at the best fit point is around 30\%, which already shows how this
analysis suffers from the many unknown optical specifications in the Monte Carlo.  The
impact of this problem on the \nnbb\ distribution analysis is addressed in
\cref{sec:2nbb-ana:systematics} through a dedicated study of the connected systematic
uncertainty. Even if these efficiencies are not directly comparable with those extracted
from physics data, a qualitative agreement is found. As a matter of fact, the event
suppression of top PMTs is generally lower than those observed for bottom PMTs and SiPMs.
A possible source of this asymmetry might be the presence of the array instrumentation
(i.e.~cabling and electronic boards) between the germanium detectors and the top PMTs.
These hardware parts, which are absent in the lower side of the LAr veto instrumentation,
represent an obstacle to the propagation of optical photons.

\blocktitle{\pdf{}s}
The background model \pdf{}s after LAr veto cut obtained by applying the probability map
to the Monte Carlo simulations are displayed in \cref{fig:bkg:lar:ph2:pdfs:gmodel}. A
notable difference with the \pdf{}s constructed for the analysis presented in
\cref{chap:bkg:raw:ph2} is that the linear transition layer model obtained from
characterization data for \bege\ detectors~\cite{Lehnert2016} is now taken into
account~---~more information about the germanium detector transition layer models is found
in \cref{apdx:gedetav}. A comparison with \pdf{}s before analysis cuts in
\cref{fig:bkg:raw:ph2:pdfs:gmodel} shows large shape deformations, especially for decays
from \Uh\ and \Thh\ chains. In particular, a large suppression of \Tl\ full-energy peak at
2614~keV is observed. The effect of the LAr veto cut is also noticeable on the Compton
shoulders, e.g.~\kvn, \kvz\ and \Co. The only \pdf\ which is practically not affected by
the cut is the \nnbb\ \pdf. The probability for the two electrons emitted in the decay to
escape from the detector and produce scintillation light in LAr is generally very low.

\begin{figure}
  \centering
   \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:Th}%
     \Bil\ and \Tl\ (\Thh\ chain) contaminations far from (fiber shroud) and close to
     (mini-shrouds) the detectors.
   ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-Th.pdf}}
   \hfill
   \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:U}%
     \Bih\ and \Pbh\ (\Uh\ chain) contaminations in the mini-shrouds.
     ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-U.pdf}}

  \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:K40}%
    \kvn\ contamination close to the detectors (mini-shrouds), at a higher radial distance
    (fiber shroud) and higher vertical distance (copper shrouds).
  ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-K40.pdf}}
  \hfill
  \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:K42}%
    \kvz\ contamination in different locations inside the LAr.
  ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-K42.pdf}} \\

   \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:other}%
   \Ac\ contamination in the close vicinity of the detectors (mini-shrouds).
   ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-misc.pdf}}
   \hfill
   \subfloat[\label{fig:bkg:lar:ph2:pdfs:gmodel:other2}%
     \Co\ contamination in the signal and high-voltage cables and detector intrinsic
     \gesix\ \nnbb\ decay.
   ]{\includegraphics[width=0.48\textwidth]{plots/bkg/lar/ph2/pdfs/gmodel-pdfs-misc2.pdf}}

  \caption{%
    The \pdf{}s for the \m{M1-AllEnr} ($\enrBEGeII + \enrCoaxII$) (in fully opaque colors) and
    the \enrGeII\ (in shaded colors) data sets after the LAr veto cut in the full energy
    domain and relative to different background sources. Bayesian blocks are used to
    visualize histograms (see \cref{apdx:bayesblocks} for details. All \pdf{}s are normalized
    to the number of simulated events. These \pdf{}s should be compared with those before the
    LAr veto cut in \cref{fig:bkg:raw:ph2:pdfs:gmodel}.
  }\label{fig:bkg:lar:ph2:pdfs:gmodel}
\end{figure}

\section{Full-range analysis}%
\label{sec:bkg:lar:ph2:gmodel}

A full-range background analysis of the energy spectra after the LAr veto cut in the same
spirit of the one reported in \cref{sec:bkg:raw:ph2:gmodel} has been carried out. The
\a-event analysis has not been repeated, as they almost completely survive the LAr veto
cut; the \pdf{}s extracted from the \a\ model presented in \cref{sec:bkg:raw:ph2:amodel} can,
therefore, be used in the full-range analysis as they are.

\blocktitle{statistical \\ methods}
The full-energy spectrum has been analyzed in its entire range from 565~keV to 5260~keV
and no data sub-sets have been excluded. To speed-up the analysis and exploit all the
information brought by data at the same time, a heuristic variable binning has been
adopted. Larger bins in the high-energy region, were the event rate is lower, have the
additional advantage of avoiding bias from low-statistics \pdf{}s. Dedicated bins have
been used for the strongest \g\ lines: the two \kvn\ and \kvz\ lines, the \Bih\ and \Tl\
lines. In the rest of the energy spectrum the bin size has been chosen according to the
event rate (e.g.~10~keV for \enrBEGeII\ and \enrCoaxII\ above the \Arl\ Q-value and below
the potassium lines, 40~keV for \enrGeII). Since above the \Tl\ full-energy peak the
\Mone\ event spectrum is dominated by \a\ events which are almost completely surviving the
LAr veto cut, only two bins $[2610, 4500]$~keV and $[4500, 5260]$~keV have been set. As a
cross-check, fit results have been tested to be stable when using this data partitioning
scheme or a fixed 5~keV binning.
\newpar
The statistical analysis to determine the background decomposition consists in the usual
Poisson likelihood maximization in a Bayesian setting presented in
\cref{sec:bkg:raw:ph2:stat}. The C++ BAT-based~\cite{Caldwell2008} software routine is
publicly available on GitHub\textsuperscript{\ref{footnote:gerda-fitter}}. Since just few
representative fit parameters are used for each radioactive contamination, no priors from
screening measurement data have been used. A uniform prior is assigned to all fit
parameters~---~including the \nnbb\ fit parameter, which is proportional to $1/\thalftwoM$.

\blocktitle{results}
Since the background level (excluding the \a\ events) on the \nnbb\ signal is about an
order of magnitude less than before the LAr veto cut, a basic sample of background \pdf{}s
has been tested in the fit model. Given the reduced size of the background sample, in
fact, no sensitivity to second-order shape distortions between \pdf{}s of the same
radioactive decay in different locations (e.g.~\Ac\ in holders, cables, etc.) is expected.
The following set of \pdf{}s has therefore been selected: \nnbb\ in germanium, \kvn\ close
(mini-shrouds) and far (fiber shroud) from the detectors, \kvz\ close (on the \bege\ and
\scoax\ \nplus\ contact, in the LAr inside the mini-shrouds) and far (in the LAr outside
the mini-shrouds, in the LAr above the array), \Bil\ and \Tl\ on cables, \Pbh\ and \Bih\
on cables, \Co\ on cables, \Ac\ on detector holders and the \a\ model \pdf{}s for \bege\
and \scoax\ detectors from \cref{sec:bkg:raw:ph2:amodel}.
\newpar
Not all the marginalized posterior distributions of fit parameters report non-zero
estimates of background counts in the spectra. Notably, posteriors for \kvz\ above the
array and on the \bege\ \nplus\ contact are peaked at zero and are thus removed from the
fit. The exclusion of \kvz\ on the \bege\ \nplus\ contact, which has not been reported in
the fit before analysis cuts (\cref{chap:bkg:raw:ph2}), is attributed to the novel
transition layer model, which significantly affects the \pdf\ shape above the full-energy
peak. The \b\ spectrum component above 1525~keV, indeed, is now higher and shows less
compatibility with experimental data. The \Co\ component is also compatible with zero,
possibly because of the high LAr veto efficiency on a background contamination which was
already low before the cut (see \cref{tab:bkg:raw:ph2:gmodel:results}). Nevertheless,
the component is not removed from the fit because of its clear non-zero mode. Finally, a
strong anti-correlation is present between the two \kvn\ sources far and close to the
array. The former can be dropped from the model without affecting the fit results.
\newpar
The results of the analysis are shown in
\cref{fig:bkg:lar:ph2:results,fig:bkg:lar:ph2:results-fixbw,fig:bkg:lar:ph2:results-closeup}
and \cref{tab:bkg:lar:ph2:results}. A close-up in the \onbb\ analysis window is shown in
\cref{fig:bkg:lar:ph2:results:roi}. Since the radioactivity of hardware parts is better
constrained within the background model before the analysis cuts
(\cref{chap:bkg:raw:ph2}), where the background level is higher, and additional
uncertainties affect the normalization of the \pdf{}s after the LAr veto cut, only the
reconstructed number of counts in the fit range and the background index are given. The
background decomposition in \cref{fig:bkg:lar:ph2:results} is presented together with the
residuals plot and the 68\%, 95\% and 99\% central Poisson probability intervals of the
best fit model. From a visual inspection of the residuals, the goodness of fit is
satisfactory through all the spectrum, with a few exceptions. In particular, discrepancies
between the best-fit model and the data are visible in \enrGeII\ right below the potassium
peaks, in \enrBEGeII\ around the potassium Compton shoulder region (see
\cref{fig:bkg:lar:ph2:results-closeup}).

\blocktitle{background \\ at \qbb}
\sloppy The background model describes the individual contributions to the total background index
around \qbb\ after the LAr veto cut. The background index is defined as the number
of counts over exposure and energy in the energy window from 1930~keV to 2190~keV
excluding the region around \qbb\ ($\qbbM \pm 5$~keV) and the intervals $2104 \pm 5$~keV
and $2119 \pm 5$~keV, which correspond to known \g\ lines from \Tl\ and \Bih. The values
for each background contribution are given in \cref{tab:bkg:lar:ph2:results}. The
composition of the background around \qbb\ is also displayed as fractions of the total
background index in the following:
\begin{center}
  \includegraphics{plots/bkg/lar/ph2/results/gmodel/BI-bar.pdf}
\end{center}
The dominating background contribution in both data sets come from
\a\ decays, which are left practically untouched by the LAr veto cut. \kvz\ is the second
largest contribution, together with isotopes from the \Uh\ decay chain.
The estimated total
background indices extracted from the marginalized posterior distributions are
$\stat{6.78^{+0.46}_{-0.42}} \cdot 10^{-3}$~\ctsper\ for the \enrBEGeII\ data set and
$\stat{5.45^{+0.28}_{-0.46}} \cdot 10^{-3}$~\ctsper\ for the \enrCoaxII\ data set.

\begin{sidewaystable}
  \footnotesize
  \centering
  \caption{%
    Summary of the background decomposition after LAr veto cut. The number of
    reconstructed counts in the fit range and the BI at \qbb\ is listed for each component
    and each analysis data set. Marginalized modes, along with its smallest 68\% C.I., are
    reported as representatives of the marginalized posterior distributions.
  }\label{tab:bkg:lar:ph2:results}
  \input{tab/bkg/lar/ph2/gmodel-results}
\end{sidewaystable}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrBEGe-results.pdf}
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrCoax-results.pdf}
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrE1plusE2-results.pdf}
  \caption{%
    The background decomposition of the first \gexpophasetwobkg\ of \gerdatwo\ after the
    liquid argon veto cut. The binning is the one used in the analysis. Contributions from
    the same radioactive decay are summed for visualization purposes. The data-to-model
    ratio is shown in the bottom panels together with the smallest 60\%, 95\% and 99\%
    confidence intervals.
  }\label{fig:bkg:lar:ph2:results}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrBEGe-results-fixbw.pdf}
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrCoax-results-fixbw.pdf}
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrE1plusE2-results-fixbw.pdf}
  \caption{%
    The background decomposition of the first \gexpophasetwobkg\ of \gerdatwo\ after the
    liquid argon veto cut with a fixed-width 15~keV binning. Contributions from the same
    radioactive decay are summed for visualization purposes. The data-to-model ratio is
    shown in the bottom panels together with the smallest 60\%, 95\% and 99\% confidence
    intervals.
  }\label{fig:bkg:lar:ph2:results-fixbw}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrBEGe-results-closeup.pdf}
  \includegraphics[width=\textwidth]{plots/bkg/lar/ph2/results/gmodel/enrCoax-results-closeup.pdf}
  \caption{%
    The background decomposition of the first \gexpophasetwobkg\ of \gerdatwo\ after the
    liquid argon veto cut in the \nnbb\ region. Contributions from the same radioactive
    decay are summed for visualization purposes. The data-to-model ratio is shown in the
    bottom panels together with the smallest 60\%, 95\% and 99\% confidence intervals.
  }\label{fig:bkg:lar:ph2:results-closeup}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{plots/bkg/lar/ph2/results/gmodel/results-roi-fixbw.pdf}
  \caption{%
    The background decomposition of the last \gexpophasetwopbkg\ of \gerdatwo\ after the
    LAr veto cut, in the \onbb\ analysis window.
  }\label{fig:bkg:lar:ph2:results:roi}
\end{figure}

\section{Discussion}%
\label{sec:bkg:lar:ph2:discussion}

As noted in the plots of the residuals shown in
\cref{fig:bkg:lar:ph2:results,fig:bkg:lar:ph2:results-fixbw,fig:bkg:lar:ph2:results-closeup},
the goodness-of-fit is acceptable through nearly all the considered energy range. Few
notable discrepancies remain, the most evident being the region below the \kvn\
full-energy peak in the \enrGeII\ data set. Since the issue is not present in the analysis
before the LAr veto cut (see \cref{sec:bkg:raw:ph2:gmodel}), where the statistics is
even higher, it could originated from an inaccurate LAr veto system modeling. The same
hypothesis might be formulated for the inconsistency between model and data observed in
the potassium Compton edge energy region in the \enrBEGeII\ data set and above the \kvz\
line in the \enrCoaxII\ data set.
\newpar
The background indices extracted from data after the LAr veto cut are
$\stat{6.2^{+0.9}_{-0.9}} \cdot 10^{-3}$~\ctsper\ for the \enrBEGeII\ data set and
$\stat{6.3^{+1.0}_{-0.9}} \cdot 10^{-3}$~\ctsper\ for the \enrCoaxII\ data set, which agree
with the ones predicted by the background model within the statistical uncertainty. The
partial contributions to the background index are compared to those found with the
background model before analysis cuts (\cref{sec:bkg:raw:ph2:gmodel}) in
\cref{fig:bkg:lar:ph2:BI-comparison}. The contribution from \a\ decays is not considered
as it is left unchanged by the LAr veto cut. The \Th\ (\Bil\ + \Tl) and
the \Co\ contributions are the most suppressed, followed by \kvz\ and \Uh\ (\Pbh\ + \Bih).
The \Ac\ contribution stays at the same negligible level as before the LAr veto cut. The
same picture is observed when comparing the count rates in the analysis energy range
(i.e.~above the \Arl\ Q-value), which are more relevant to the \nnbb\ analysis, in
\cref{fig:bkg:lar:ph2:cts-comparison}, with some minor differences.

\begin{figure}
  \centering
  \includegraphics{plots/bkg/lar/ph2/results/gmodel/raw-vs-lar-cts-comparison.pdf}
  \caption{%
    Comparison between total counts in the fit range (i.e.~above 565~keV) obtained from
    the background model before analysis cuts and after LAr veto cut. The background
    decompositions are performed with a reduced set of \pdf{}s, to make a one-to-one
    comparison possible.
  }\label{fig:bkg:lar:ph2:cts-comparison}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{plots/bkg/lar/ph2/results/gmodel/raw-vs-lar-BI-comparison.pdf}%
  \vspace{0.5cm}
  \begin{tikzpicture}[anchor=south west]
    \node at (0, 2.5) {\scshape Before analysis cuts};
    \draw    (0, 2.5) -- (\textwidth,2.5);
    \node at (0, 0.0) {\includegraphics{plots/bkg/raw/ph2/results/gmodel/BI-bar.pdf}};

    \node at (0,-1.0) {\scshape After the LAr veto cut};
    \draw    (0,-1.0) -- (\textwidth,-1);
    \node at (0,-4.6) {\includegraphics{plots/bkg/lar/ph2/results/gmodel/BI-bar-norm.pdf}};
  \end{tikzpicture}
  \caption{%
    Comparison between background indices obtained from the background model before
    analysis cuts and after LAr veto cut, for the \enrBEGeII\ and \enrCoaxII\ data sets.
    The background decompositions are performed with a reduced set of \pdf{}s, to make a
    one-to-one comparison possible. On the top, the background index values are reported
    with statistical uncertainties extracted from the fit. On the bottom, bars are used to
    represent the fractional composition. The scale in units of \powctsper{-3} is reported
    at the bottom end.
  }\label{fig:bkg:lar:ph2:BI-comparison}
\end{figure}

\chapendgliph{}

\chapsummary
\begin{itemize}
  \item The background model \pdf{}s after the LAr veto cut necessitates the implementation
    of the LAr veto instrumentation as well as the optical properties of all materials
    involved in the propagation of the scintillation photons into the Monte Carlo
    framework (\mage). Many of these properties are unfortunately not precisely known: the
    LAr absorption length, the channel efficiencies, the germanium reflectivity in the VUV
    light regime, among the others.
  \item The high computational time cost of the \geant\ simulations of optical processes
    require the implementation of an alternative approach to compute the LAr veto flag for
    Monte Carlo events, rather than directly enabling the optical physics in the \mage\
    simulations. A three-dimensional LAr light detection probability map is independently
    built by means of a dedicated, massive simulation of scintillation photons in the LAr.
    The map is then convoluted with the existing background model simulations to determine
    the LAr veto flag.
  \item Since the shape of the \nnbb\ distribution is nearly not affected by the LAr veto
    and the background level is low, a qualitative LAr probability map is enough in light
    of the \nnbb\ energy distribution analysis after the LAr veto cut. Special calibration
    data with LAr veto discrimination is used to determine three average detection
    efficiencies for top PMTs, SiPMs and bottom PMTs. The background model \pdf{}s are
    expected to be negligibly affected by differences between single-channel efficiencies.
    The impact of other possible systematic uncertainties in the LAr veto modeling is
    assessed in the context of the \nnbb\ shape analysis.
  \item The first \gexpophasetwobkg\ of \phasetwo\ data after the LAr veto cut is divided
    into the usual three data sets corresponding to single-detector events in \bege\ and
    \scoax\ detectors and two-detector events in all enriched detectors. Their energy
    spectrum is decomposed into background and signal (\nnbb) components in the same
    statistical framework presented in \cref{chap:bkg:raw:ph2}. The goodness-of-fit is
    overall acceptable but systematic deviations are present in selected regions of the
    spectrum. They can be due to uncertainties in the Monte Carlo LAr veto model.
\end{itemize}

% vim: tw=90
