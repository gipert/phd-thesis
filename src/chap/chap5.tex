%!TEX root = ../main.tex

\chapter{\texorpdfstring{Precision \nnbb\ distribution analysis}{Precision 2νββ distribution analysis}}%
\label{chap:2nbb-ana}

As demonstrated in \cref{chap:theory}, the \nnbb\ event distribution is of
great interest for new-physics searches. Many of these exotic processes can indeed
generate distortions in the energy spectrum shape predicted by the Standard Model. The
most frequently-considered phenomena, namely neutrinoless double-beta decay with Majoron
emission (\onbbx, \onbbxx) and Lorentz-violating two-neutrino double-beta decay (\nnbblv)
have been reviewed in \cref{chap:theory}. The aim of the research presented in this
chapter is to constrain the presence of these distortions in the \nnbb\ events collected
by \gerda\ in the first part of \phasetwo, by setting limits on the theoretical model
parameters that regulate their magnitude. Moreover, a new estimate of the \nnbb\ half-life
\thalftwo\ will be presented with a significantly reduced systematic uncertainty budget
compared to previous publications~\cite{Agostini2015a}. To improve the sensitivity of the
analysis, the data after the liquid argon veto cut is considered for the first time. As
already shown in \cref{sec:bkg:lar:ph2:gmodel}, in fact, the signal-to-background ratio
improves by a factor of 10 in the \nnbb\ energy region after the cut. Furthermore, to
reduce the systematic uncertainties connected to the detector active volume model, data
from enriched semi-coaxial detectors is discarded. The low background level
(signal-to-background ratio of $\sim$20 in the \nnbb\ region excluding the potassium \g\
lines) and the high-statistic signal data sample ($\sim$$5 \cdot 10^{4}$~\nnbb\ events
above the \Arl\ Q-value, see \cref{tab:bkg:lar:ph2:results}) motivates the construction of
a high-precision fit of the \nnbb\ energy distribution to test the validity of the
Standard Model predictions.
\newpar
This chapter is divided in three sections: the data selection and the statistical methods
which are used to study the signal sensitivity and analyze the data set are presented in
\cref{sec:2nbb-ana:stat}, while sources of systematic uncertainties and their effect on
the analysis are extensively described in \cref{sec:2nbb-ana:systematics}. The
results are finally presented and discussed in \cref{sec:2nbb-ana:results}.

\section{Statistical analysis}%
\label{sec:2nbb-ana:stat}

\blocktitle{data and \\ \pdf{}s}
The \enrBEGeII\ data set after the LAr veto cut (already characterized in
\cref{sec:bkg:lar:ph2:data}, see \cref{fig:bkg:lar:ph2:data-desc} (top panel) for the
energy spectrum) has been considered for this analysis both because of the higher
signal-to-background ratio in the \nnbb\ region of about 20 (excluding the two potassium
\g\ lines) compared to data before analysis cuts and for the lower uncertainty in the
\bege\ detectors active volume determination (see \cref{apdx:gedetav}).
\newpar
The theoretical predictions for signal and background event distributions are obtained, as
usual, from Monte Carlo simulations through the \mage\ software framework. The LAr veto
model, which is extensively described in \cref{sec:bkg:lar:ph2:pdfs}, is used to compute
the LAr veto flag for synthetic events. A selection of \pdf{}s is shown in
\cref{fig:bkg:lar:ph2:pdfs:gmodel}. The same \pdf\ sample considered in the background model
after the LAr veto cut (\cref{sec:bkg:lar:ph2:gmodel}) has been selected to represent the
background in this analysis. The choice of a reduced set, compared to the background model
before analysis cuts, is motivated by the fact that the description of the shape of such a
low background does not benefit from more complex models, from a statistical point of
view. The goodness-of-fit for the \enrBEGeII\ data set in the \nnbb\ region is,
indeed, satisfactory (see \cref{fig:bkg:lar:ph2:results-closeup}), and the
impact of different \pdf\ shapes (e.g.~\Ac\ far or close to the detector array) is assessed
in the analysis of the systematic uncertainties. Note that the information from the
\enrCoaxII\ and \enrGeII\ data sets is also not included in the present analysis.
\newpar
The likelihood function that brings data and expectations together is the usual Poisson
likelihood, which runs over the binned \enrBEGeII\ energy spectrum:
\[
  \mathcal{L}(S, \vec{B} | \vec{n}) =
    \prod_i^{N} \frac{{\nu_i(S, \vec{B})}^{n_i} e^{\nu_i(S, \vec{B})}}{n_i!} \;,
\]
where $i$ is the bin index, $N$ is the number of bins, $n_i$ is the number of counts
observed in bin $i$ and $\nu_i$ is the predicted number of counts in bin $i$. The latter
can be decomposed as
\[
  \nu_i(S, \vec{B})
    = s_i + \sum_k b_{i,k}
    = S \int_i \text{PDF}_S(E)dE + \sum_k B_k \int_i \text{PDF}_{B_k}(E)dE \;,
\]
where $s_i$ and $b_{i,k}$ are the signal (\nnbb\ or new physics) and background
contribution from component $k$ in bin $i$ and the integrals are defined in the energy
interval spanned by the bin. $S$ and $B_k$ represent the total number of counts from
signal and background events in the analysis range, respectively. The parameter of
interest in this analysis is, of course, $S$, and the $B_k$ are treated as nuisance
parameters. The relation between number of signal counts $S$ and the corresponding process
half-life can be expressed as:
\begin{equation}\label{eq:2nbb-ana:halflife}
  \thalftwoM = \mathcal{C}_{76} \frac{1}{S}\;;\quad
    \text{where}\; \mathcal{C}_{76} = \frac{\mathcal{N}_A\, \log2}{M_{76}}
      \left( \sum_i m^\text{tot}_i \, f^\text{AV}_i \, t_i \,
      \epsilon_{\text{c},i} \right) f_{76} \, \epsilon_\text{QC} \, \epsilon_\text{LAr} \;,
\end{equation}
$\mathcal{N}_A$ is the Avogadro number, $M_{76}$ is the \gesix\ molar mass,
$m^\text{tot}$ is the total detector mass, $f^\text{AV}_i$ is the active volume fraction,
$t_i$ is the detector live-time, $\epsilon_{\text{c},i}$ is the detector containment
probability, $\sum_i$ runs over the detectors included in the data set, labeled by index
$i$, $f_{76}$ is the enrichment fraction, $\epsilon_\text{LAr} = (97.7 \pm 0.1)\%$ is
the probability for a signal event to survive the LAr veto cut and $\epsilon_\text{QC} =
(99.922 \pm 0.002)\%$ is the quality cuts signal efficiency.

\blocktitle{test \\ statistic}
To test a hypothesized value of $S$ the following profile likelihood ratio is defined:
\[
  \lambda(S) = \frac{\mathcal{L}(S, \hat{\hat{\vec{B}}})}{\mathcal{L}(\hat{S}, \hat{\vec{B}})}
\]
where $\hat{\hat{\vec{B}}}$ denotes the value of $\vec{B}$ that maximizes $\mathcal{L}$
for the specified $S$ and $\hat{S}$, $\hat{\vec{B}}$ are maximum likelihood
estimators. The test statistic is defined as
\[
  t_S = -2\log\lambda(S)
\]
where higher values of $t_S$ correspond to increasing incompatibility between the data and
$S$. It is a known result of the Wilks theorem that the probability distribution of the
test statistic $t_S$ follows, in the large sample limit, a $\chi^2$ distribution with
number of degrees of freedom given by the number of parameter of
interest~\cite{Cowan2011}.  Since, however, not all the regularity conditions for the
Wilks theorem to hold~\cite{Algeri2020} are satisfied in this analysis, deviations from
the $\chi^2$ distribution are expected. Therefore, the $t_S$ distribution is computed from
Monte Carlo toy experiments, in which synthetic energy spectra are generated from the
background model results (\cref{sec:bkg:lar:ph2:gmodel}). For each signal hypothesis,
$10^4$ toy data sets are generated and an histogram is filled with the corresponding
values of $t_S$. Examples for the standard \nnbb\ signal hypothesis and new-physics
null hypotheses are shown in \cref{fig:2nbb-ana:ts-dist}. Deviations from the $\chi^2$
distribution for 1 degree of freedom (left panel) are observed for the standard \nnbb\
hypothesis. In the analysis all the fit parameters are constrained to be positive. In the
case of new physics hypotheses, where the maximum likelihood estimator of $S$ is close to
zero, this assumption modifies the distribution of the test statistic (right panel). In
the large sample limit it is expected to be well approximated by a half-$\chi^2$
distribution~---~a sum of a delta function at zero and a $\chi^2$ distribution for one
degree of freedom~\cite{Chernoff1954}.
\newpar
The \pvalue\ of an observed value of the test statistic $t_S^\text{obs}$ under the signal
hypothesis $S$ is used to quantify the level of disagreement between the data and the
hypothesis:
\[
  p = \int_{t_S^\text{obs}}^\infty f(t_S | S) dt_S \;.
\]
In particular, a map between the observed $t_S^\text{obs}$ and the signal level $S$
corresponding to a certain confidence region (e.g.~68\% C.L.~or $p=0.32$) can be computed
using the above equation.

\begin{figure}
  \centering
  \includegraphics{plots/2nbb-ana/2nbb-teststat.pdf}
  \caption{%
    Sampling distribution of the test statistic probability $P$ for various signal
    hypothesis sampled with Monte Carlo methods. On the left: the standard \nnbb\ case. A
    signal of $S \sim 4.5 \cdot 10^5$ counts in the analysis range is assumed in the toy
    experiments. On the right: sampling distributions for the null hypothesis on
    new-physics signals.  Deviations from the Wilks theorem predictions are observed.
    Bayesian blocks have been used to represent the histograms (see
    \cref{apdx:bayesblocks}). \fillme{update, add LV}
  }\label{fig:2nbb-ana:ts-dist}
\end{figure}

\begin{figure}
  \centering
  \includegraphics{plots/2nbb-ana/pvalues.pdf}
  \caption{%
    Median \pvalue\ of the null hypothesis with different signal strength assumption for
    the new-physics signals, computed with Monte Carlo toy experiments. In the right
    plot, the effect of additional systematic uncertainties is included. The \pvalue\
    corresponding to 90\% confidence level is highlighted with dashed lines. \fillme{update}
  }\label{fig:2nbb-ana:pvalues}
\end{figure}

\blocktitle{sensitivity \\ studies}
The sensitivity to alternative signal hypotheses can be characterized by the median
significance, assuming data generated according to the zero-signal (null) hypothesis, with
which one rejects the signal hypothesis~\cite{Cowan2011}. To compute the sensitivity, the
distributions $f(t_S|S)$ and $f(t_S|0)$ are needed. For a discrete set of hypothesis on
the signal $S$, the two distributions are obtained with Monte Carlo toy experiments and
the median \pvalue\ of null hypothesis is computed. In \cref{fig:2nbb-ana:pvalues} the
median \pvalue\ as a function of $S$, for all the considered new-physics processes is
shown. The sensitivity for 90\% C.L.~limit setting corresponds to the value of $S$ for
which the median \pvalue\ is $0.1$. The limit-setting sensitivities to the half-lives of
the investigated processes are reported in \cref{tab:2nbb-ana:sensitivity}.

\blocktitle{fit range \\ and \\ binning}
The fit range is chosen in order to maximize the signal-to-background ratio and the
sensitivity. It starts from the \Arl\ Q-value at 565~keV and stops at the \nnbb\ Q-value
(2039~keV). The \Arl\ background, as a matter of fact, is still dominant after the LAr
veto cut and reduces the signal-to-background ratio to less than 1 in its energy domain.
On the other hand, extending the fit range above the \nnbb\ Q-value does not improve the
signal sensitivity.
\newpar
A 10~keV binning, given the energy resolution of the \enrBEGeII\ data set, does not
remove physical features in the spectrum and is large enough to avoid effects due to
energy scale-related systematic uncertainties. Different bin sizes are tested
to check that the performance of the fit is not affected by this choice.

\begin{table}
  \centering
  \caption{%
    Sensitivity for 90\% C.L.~limit setting on the half-lives of new physics processes
    contributing to the \nnbb\ event distribution, before and after the inclusion of
    systematic uncertainties. The results are extracted from toy Monte Carlo data sets.
    \fillme{numbers}
  }\label{tab:2nbb-ana:sensitivity}
  \begin{tabular}{lcrlrl}
    \toprule
                           &           & \mc{4}{Sensitivity}                                                    \\
    \cmidrule(lr){3-6}
    Decay                  & Spectral  & \mc{2}{Statistical}                & \mc{2}{With systematics}          \\
    % \cmidrule(lr){3-4}\cmidrule(lr){5-6}
    Mode                   & index $n$ & Counts & $T_{1/2}$ (\powtenyr{23}) & Counts & $T_{1/2}$ (\powtenyr{23})\\
    \midrule
    \onbbx\                & 1         &    171 & 7.9                       &    241 & 5.6                      \\
    \onbbx\                & 2         &        &                           &        &                          \\
    $\onbbM\upchi(\upchi)$ & 3         &        &                           &        &                          \\
    \onbbxx\               & 7         &        &                           &        &                          \\
    \nnbblv\               & 4         &        & --                        &        & --                       \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Systematic uncertainties}%
\label{sec:2nbb-ana:systematics}

Besides the purely-statistical effects, a set of uncertainties which might contribute
systematically to the final analysis uncertainty (both for the \nnbb\ half-life and
new-physics limits) must be considered. Until now, the test statistics studies have been
considering only the effect of Poisson fluctuations in the bin contents, as the generative
model for the toy data sets was fixed. A way to include systematic model uncertainties
when sampling the test statistic distribution is to sample the generative \pdf\ from a set
of `alternative' models according to a certain probability distribution. This is
conceptually equivalent to fitting the data with `wrong' models that assume the presence
of a systematic distortion of the best model. As instance, alternative transition layer
or LAr veto models induce coherent distortions in all background and signal \pdf{}s, which
must be taken into account when generating the toy data sets.
\newpar
This way of treating systematic uncertainties is formalized as a \emph{hybrid
Bayesian-frequentist} approach~\cite{Zyla2020}. In this setting, the distribution
of the test statistic becomes:
\[
  f(t_s) = \int f(t_s | S, \vec{B}, \nu) \pi(\vec{\nu}) d\vec{\nu} \;,
\]
where $\vec{\nu}$ are the parameters representing the sources of systematic uncertainties
in the model and $\pi(\vec{\nu})$ the `prior' distribution from which $\vec{\nu}$ is
sampled from. The effect introduced by these additional parameters is to smear $f(t_S)$,
weakening the new-physics experimental limits extracted from it. The software that
implements this approach for the \nnbb\ analysis is implemented in the
\texttt{gerda-factory} suite, publicly available on GitHub\footnote{%
  The \texttt{gerda-factory} software suite, available at
  \url{https://github.com/gipert/gerda-factory}, Is implemented in C++ and externally
  depends only on the ROOT libraries for the histogram utilities. The program input is
  specified in JSON format. First, a reference model (in the form of a linear combination
  of histograms, i.e.~the signal \pdf, the background \pdf{}s etc.) is set, then the program
  is fed with alternative \pdf\ shapes (histograms) for each of the model components. These
  are grouped according to the source of systematic uncertainty they represent: they can
  be provided for all components at the same time (global distortions) or for selected
  \pdf{}s (specific distortions). The program then computes and stores distortion curves
  ($\text{pdf}_\text{orig}/\text{pdf}_\text{dist}$) for each alternative \pdf\ shape. At run
  time, a distortion curve is randomly selected from each group and applied to the
  reference model; a random data set is then drawn from the obtained model. The procedure
  is repeated in order to generate the required number of randomly-distorted synthetic
  data sets. Optionally, as the alternative \pdf\ shapes are discrete, the program can be
  instructed to consider intermediate distortion curves by interpolation.
}. In the following, the main sources of systematic uncertainties impacting the \nnbb\
analysis are discussed.

\begin{description}[wide]

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{plots/2nbb-ana/larmap-dist-manual.pdf}
      \caption{%
        Effect of power-law distortions of the LAr probability map in a cross-section
        parallel to the $xy$ plane. The value of the exponent $a$ in the $p \rightarrow
        p^a$ transformation is varied to make the map more or less homogeneous.
      }\label{fig:2nbb-ana:larmap-dist-showoff}
    \end{figure}

  \item[LAr veto model] The Monte Carlo LAr veto model, as shown in detail in
    \cref{sec:bkg:lar:ph2:pdfs}, suffers from many uncertainties, some of them arising
    directly from the poor knowledge of LAr channel efficiencies and material optical
    properties implemented in \mage\ (c.f.r~\cref{sec:apdx:mage-optics}). The systematic
    effect of variations of some of these Monte Carlo parameters (i.e.~the LAr absorption
    length, the germanium reflectivity, the coverage of the fiber shroud and the TPB
    quantum efficiency) has been already studied in selected regions of the LAr
    probability map (the object in which the LAr veto model is encoded) in
    \cref{sec:bkg:lar:ph2:heatmap,fig:bkg:lar:ph2:larmap:dist}. Special calibration data
    can be used to determine the channel efficiencies (\cref{sec:bkg:lar:ph2:pcalib}), but
    could not be exploited to reliably constrain other Monte Carlo parameters. This second
    possibility, which requires a much more complex analysis and a deeper understanding of
    simulated and physics data, is discussed in~\cite{Wiesinger2021}.
    \newpar
    To include the LAr veto model uncertainties in the \nnbb\ distribution analysis, the
    following approach has been formulated. First, the special calibration data is
    compared to Monte Carlo simulations to extract three effective channel efficiencies:
    one for all top PMTs, one for all SiPM modules and one for all bottom PMTs
    (\cref{eq:bkg:lar:ph2:chan-eff}). Considering the cylindrical symmetry of the problem
    (a set of detectors arranged in a cylindrical array), A more detailed knowledge of
    efficiencies for each single channel is not necessary. In this reference probability
    map the other Monte Carlo optical parameters are fixed to the values that better
    reflect our degree of belief, which are documented in \cref{sec:apdx:mage-optics}.
    \newpar
    The second step is to provide alternative LAr veto maps for the determination of the
    test statistic distribution, generated with different assumptions for the optical
    parameters. These alternative maps will still have to reproduce the special
    calibration data, i.e.~the \emph{control sample}, but they will differ from the
    original map in all other LAr regions, in particular those probed by the background
    model simulations.  Unfortunately, the map generation process is exceptionally
    expensive from the computational point of view (the required time is on the order of
    several thousands of CPU hours), therefore obtaining several alternative probability
    maps from scratch is not feasible. To overcome this issue, the possibility to perform
    manual distortions of the reference probability map has been investigated.
    \newpar
    An alternative presentation of the probability map distortions in
    \cref{fig:bkg:lar:ph2:larmap:dist}, in which the detection probability in the first
    calibration source position area is set to unity, is given here in
    \cref{fig:2nbb-ana:dist-pcanorm} for the germanium reflectivity, the fiber shroud
    coverage and the LAr absorption length (first three panels from left). In this way the
    constraint of reproducing the special calibration data set is made visually explicit,
    as the corresponding probability (in red) is invariant under transformations of the
    Monte Carlo parameters. TPB quantum efficiency and LAr light yield distortions are not
    considered because they can be well-approximated with a global scaling of the
    probability map (i.e.~their effect is fully absorbed in the LAr channel efficiencies).
    The distortions shown in the rightmost panel are obtained in a different way: instead
    of running the full simulation chain with different Monte Carlo parameters to compute
    the detection probability in the selected spatial points, the probability map is
    directly distorted by means of an analytical transformation. A power-law
    transformation is used:
    \begin{equation}\label{eq:2nbb-ana:powerlaw}
      p_k \rightarrow c \cdot p_k^a \;,
    \end{equation}
    in which $p_k$ is the probability value in the LAr voxel $k$ (i.e.~the probability
    map) and $a$ is an arbitrary coefficient controlling the magnitude of the distortion
    and $c$ is a global normalization factor. The latter depends on $a$ and has to be
    adjusted in order to reproduce the total LAr veto event suppression observed in the
    special calibration data. In this sense, the volume of LAr probed by the calibration
    data is a fixed point of the transformation in \cref{eq:2nbb-ana:powerlaw}, and the
    magnitude of the distortion in all other regions is regulated by $a$. The effect of
    the transformation on a transversal cross-section of the probability map is shown in
    \cref{fig:2nbb-ana:larmap-dist-showoff}.
    \newpar
    As it can be stated from \cref{fig:2nbb-ana:dist-pcanorm}, the power-law distortions
    shown in the rightmost panel are not radically different from e.g.~those generated by
    the reflectivity changes. Moreover, the latter is arguably the most important source
    of systematics in the \nnbb\ analysis, since it probes the LAr volume enclosed by the
    detectors. The power-law map distortion has been therefore judged representative
    of the class of distortions generated by fundamental Monte Carlo parameters\footnote{%
      It is not difficult to realize that probability map distortions generated by
      fundamental Monte Carlo properties cannot be generally reproduced by global map
      transformations. The effect of these parameters is indeed predominantly local: the
      germanium reflectivity will affect the array region, the fiber shroud coverage the
      region close to the shroud etc. Also the LAr absorption length effect might be
      localized, depending on the scintillation photon mean free path in a certain LAr
      region.  Based on this observation, more complex direct distortions of the map
      limited to certain locations could be also considered. Nevertheless, given the
      expected low impact of LAr veto model uncertainties on the \nnbb\ analysis, this
      option has not been further explored.
    }.
    \newpar
    A set of alternative background \pdf\ shapes has been generated with power-law distorted
    maps, varying $a$ from 0.5 to 1.5 in steps of 0.1. The effects on the energy
    spectra of \kvn\ on cables and \kvz\ in LAr have been reported in
    \cref{fig:2nbb-ana:pdf-dist} (left panel), as an example. As it is clear, the
    distortion strongly depends on the energy and the type (Compton scattering, full
    absorption) of the event. All these alternative \pdf{}s have been considered in the
    determination of the distribution of the test statistics, with a flat prior.

    \begin{figure}
      \centering
      \includegraphics{plots/2nbb-ana/larmap-dist-pcanorm.pdf}
      \caption{%
        Study of the impact of Monte Carlo parameters on LAr light detection probabilities in
        various spatial points, normalized to unity at the best value and such to leave the
        probability at the uppermost calibration source position (red point) unchanged. In
        the rightmost panel the effect of a direct, power-law transformation of the
        probability map is shown (no normalization to the red point applied, see text for
        details). The color code and other details are described in
        \cref{fig:bkg:lar:ph2:larmap:dist}.
      }\label{fig:2nbb-ana:dist-pcanorm}
    \end{figure}

  \item[\gesix\ active exposure] The detector active volume and enrichment fraction has
    been estimated for \bege\ detectors up to a certain accuracy during a dedicated
    characterization campaign~\cite{Agostini2015e, Agostini2019}. The enriched active
    volume determines the total amount of detected \nnbb\ counts, and it is therefore not
    expected to produce distortions in the shape of the \pdf{}s. Its effect must be
    considered in the conversion of the number of signal counts to the corresponding
    process half-life, expressed in \cref{eq:2nbb-ana:halflife}.  Since three independent
    estimates of the \bege\ enrichment fraction $f_{76}$ are
    available\cite{Agostini2015e},
    they are combined into the final estimate of 0.877(13) by evaluating their variance
    (see \cref{tab:gerda:efficiencies}). The estimate of the full charge-collection depth
    and size of transition region for each detector required a careful analysis of the
    detector characterization data and is reviewed in
    \cref{apdx:gedetav}~\cite{Agostini2019, Lehnert2016}. The uncertainty on these values
    is statistically propagated to the half-life conversion factor $\mathcal{C}_{76}$ by
    coherently treating with Monte Carlo methods the correlated and un-correlated
    uncertainties in the linear combination $\sum_i$ of the active volume fraction
    $f^\text{AV}$ in \cref{eq:2nbb-ana:halflife}. The resulting uncertainty is about 2\%
    of $\mathcal{C}_{76}$. Considering that it propagates almost linearly to the final
    half-life estimate, the \gesix\ active exposure estimation results to be the major
    source of systematic uncertainty in the standard \nnbb\ half-life determination.

  \item[Transition layer model] As already discussed in several occasions (see in
    particular \cref{apdx:gedetav}), the transition region from the \nplus\ electrode to
    the fully-active detector volume is not completely dead (i.e.~the charge-collection
    efficiency is not zero). Since events interacting with this special region are
    reconstructed with a smaller effective energy, an effect on the energy distribution
    shape is expected. Distortions induced by variations of the size of the transition
    layer are particularly noticeable in the \Arl\ event distribution
    (\cref{fig:gedetav:ar39:distortions}) and in the lower tail of intense \g\ peaks
    (\cref{fig:gedetav:calib-optim-example}). A first estimate of the size of the
    transition region for \bege{}s has been extracted from detector characterization
    data~\cite{Lehnert2016}, using a simplified linear model for the charge-collection
    efficiency profile (\cref{fig:gedetav:tl-model}). This estimate, however, has been
    obtained before a significant amount of storage time at room temperature (2--3 years
    before deployment in LAr), in which \nplus\ Lithium profile growing effects took
    place. The effect of this dead-layer growing process on the size of the transition
    region is unfortunately unknown. The assumption of a fixed proportion between the size
    of the transition region and the size of the dead region during the growing process
    leads to results which are clearly incompatible with experimental data (see
    e.g.~\cref{fig:gedetav:calib-optim-example} and \cref{fig:gedetav:ar39-results}).
    \newpar
    To provide a more reliable \bege\ detector transition layer model for the \nnbb\
    analysis, an independent analysis of the \Th\ full-energy-peak lower tail from
    calibration data has been performed and documented in detail in
    \cref{sec:gedetav:calib-optim}. A best-fit value of the dead-layer fraction with
    statistical uncertainty has been obtained for each detector. \pdf{}s for signal and
    background sources corresponding to the best-fit values $\pm1\sigma$, $\pm2\sigma$ and
    $\pm5\sigma$ ($\sigma$ denotes the standard deviation) have been produced and
    reported in \cref{fig:2nbb-ana:pdf-dist} (right panel) for \kvn, \kvz\ and \nnbb\ in
    the \enrBEGeII\ data set. The \pdf\ distortions are noticeably lower than those induced
    by the LAr veto model uncertainties, and are found to be negligible in the total
    \thalftwo\ systematic uncertainty count.

    \begin{figure}
      \centering
      \includegraphics{plots/2nbb-ana/bege-pdf-dist.pdf}
      \caption{%
        Effect of alternative LAr veto models (right) and transition layer models (left) on
        \kvn\ (signal and high-voltage cables), \kvz\ (homogeneous in the LAr) and \nnbb\
        \pdf{}s for the \enrBEGeII\ data set after the LAr veto cut. See text for details on how
        the alternative models have been constructed.
      }\label{fig:2nbb-ana:pdf-dist}
    \end{figure}

  \item[Background model] The uncertainty on the location of the main background sources
    is treated as a systematic uncertainty source in the analysis. \pdf{}s for the same
    radioactive contamination in different locations are provided as an alternative to the
    reference model \pdf. In the following table the alternative locations selected for the
    evaluation of the systematic contribution are listed for each fit model component (the
    reference location in \emph{italic}):
    \begin{center}
      \begin{tabular}{ll}
        % \toprule
        Component      & Location                                                     \\
        \midrule
        \mr{2}{\kvn}   & \emph{mini-shrouds}, cabling, holder mounting,               \\
                       & front-end electronics, copper shroud                         \\
        \kvz\ (close)  & \emph{inside mini-shrouds}, \nplus\ contact, \pplus\ contact \\
        \kvz\ (far)    & \emph{outside mini-shrouds}, above the array                 \\
        \Ac\           & \emph{holder mounting}, fiber shroud                         \\
        \Bil\ + \Tl\   & \emph{cabling}, outer fibers                                 \\
        % \bottomrule
      \end{tabular}
    \end{center}
    Some of these \pdf{}s are shown in \cref{fig:bkg:lar:ph2:pdfs:gmodel}. The contribution
    of background modeling-related uncertainties to the total \nnbb\ half-life uncertainty
    amounts to 0.5\%.

  \item[Theoretical \nnbb\ decay model] The theoretical calculations of the \nnbb\
    distribution shape are not exact and are based on approximations and assumptions
    (\cref{chap:theory}).  \nnbb\ primary vertices are generated in \mage\ through the
    \decayzero\ program, whose theoretical formulae are documented
    in~\cite{Ponkratenko2000} and references therein.  In similar works in the
    past~\cite{Agostini2015a, Agostini2012a} the systematic uncertainty contribution
    arising from the decay theoretical description has been evaluated by considering the
    \nnbb\ distribution obtained with the Primakoff-Rosen
    approximation~\cite{Primakoff1959} as an alternative. The comparison resulted in
    a~$<1$\% effect, which was negligible in the total uncertainty budget.  There is,
    however, no guarantee that the impact on the current analysis is at the same,
    second-order, level. The lower background and higher statistics compared to the
    \phaseone\ data set makes a re-evaluation of this systematic contribution necessary.
    Calculations in the Primakoff-Rosen approximation, however, are known to yield results
    which are inexact (see \cref{fig:2nbb-ana:theoretical-spectrum}). Results from recent
    calculations, e.g.~phase space factors calculated with exact Dirac wave functions with
    finite nuclear size and electron screening effects~\cite{Kotila2012}, are more
    suitable to this comparison. As a matter of fact, the calculations implemented in
    \decayzero\ and those presented in~\cite{Kotila2012} yield nearly identical results.
    Therefore, no contribution to the systematic uncertainty is expected, from this side.
    \newpar
    Another source of theoretical uncertainty which has recently turned out to be
    relevant~\cite{Arnold2019, Azzolini2019a} is the assumption on the nuclear state
    configuration of the intermediate nucleus in the decay (\nuc{As}{76} for \nnbb\ decay
    of \gesix). In \gesix\ a higher-state dominance (HSD) is usually assumed, but a
    single-state dominance (SSD) cannot be excluded. Summed-energy spectra obtained with
    the two hypotheses, using exact Dirac wave functions with finite nuclear size and
    electron screening effects~\cite{Kotila2012}, are compared in
    \cref{fig:2nbb-ana:theoretical-spectrum}. Once the distributions are normalized to
    unit area, the difference is mostly visible above $\sim$1.3~MeV, where it reaches 5\%
    and more. In this region, however, the event statistics is also low. Detailed studies
    are ongoing to assess whether the \nnbb\ shape analysis is sensitive to this
    discrepancy. For now, a conservative $<0.1$\% contribution to the total uncertainty
    budget is assumed.

    \begin{figure}
      \centering
      \includegraphics{plots/2nbb-ana/2nbb-sums-assumptions.pdf}%
      \includegraphics{plots/2nbb-ana/2nbb-sums-assumptions-zoom.pdf}
      \caption{%
        Comparison between different theoretical assumption in the calculation of the
        phase space factor for the standard \nnbb\ decay. Results of the calculations with
        the Primakoff-rosen approximation, the higher-state dominance (HSD) and
        single-state dominance (SSD) are shown, normalized to unit area. The HSD curve is
        implemented in \decayzero\ and used as the reference spectrum in the \nnbb\
        analysis.
      }\label{fig:2nbb-ana:theoretical-spectrum}
    \end{figure}

  \item[\mage\ and \geant{}] Another possible source of systematic biases in the analysis
    is the implementation of the experimental setup into the Monte Carlo software stack
    (\mage) and the implementation of the physics processes (particle generation and
    propagation) in \geant. Starting from the first point, uncertainties in the dimension
    and position of the implemented setup components are potentially present in \mage, and
    could in principle affect the shape of the signal and background \pdf{}s. Because of the
    point-like topology of the \nnbb\ decay and uniformity of the \gesix\ isotopic
    fraction, its \pdf\ is expected to depend on the total detector volume and active volume
    model, rather than the exact dimensions or location. The effect of this type of
    uncertainty source has been already described above. On the other hand, the shape
    of the background \pdf{}s is expected to depend more on the details of the geometrical
    implementation. This kind of effect, however, is partly similar to (and certainly less
    intense than) the one produced by background model-related uncertainties (see above).
    Moreover, given the modest size of the background sample, it is expected to contribute
    with less than 0.1\%. Past evaluations showed a $\sim$1\% impact on the \nnbb\
    half-life estimate~\cite{Agostini2012a, Agostini2015a}. The size of the
    background sample after the LAr veto cut, a factor $\sim$10 less than before the cut,
    and the correlation with the background modeling uncertainties, motivates the
    reduction of this contribution by one order of magnitude.
    \newpar
    \geant{} is a broadly-used and heavily-tested software suite in the high-energy
    physics community. No systematic biases due to the implementation of particle creation
    and propagation routines in the code are expected, however there could be some
    originating from uncertainties in the experimental data on which \geant\ relies on
    (i.e.~cross sections and decay widths). To test this eventuality, the full simulation
    chain has been re-run with different electromagnetic low-energy process models
    available in \geant\ (\texttt{Livermore} is the default, alternatives are
    \texttt{Penelope} and \texttt{Standard\_opt3}). The \pdf{}s are observed to change at the
    sub-percent level, and the associated contribution to the systematic uncertainty is
    negligible\footnote{%
      In support of this conclusion, the reader is referred to the official \geant\
      validation portal at \url{https://geant-val.cern.ch}, that shows how precise the
      software is in reproducing the experimental cross-section data in the energy domain
      of the typical \gerda\ backgrounds.
    }. A drastic reduction of the impact of this systematic uncertainty compared to
    previous estimates~\cite{Agostini2012a} is also motivated by the lower background
    level. In this particular application, as a matter of fact, the Monte Carlo
    uncertainty is mainly due to the propagation of the external \g\ rays: the \nnbb-decay
    electrons generated in the germanium detectors have a sub-cm range and they usually
    deposit their entire kinetic energy, apart from small losses due to the escape of
    Bremsstrahlung or fluorescence photons.

  \item[Other sources] The energy scale and resolution is another potential source of
    bias in the analysis. Given, however, the excellent resolution of germanium detectors
    and the remarkably precise knowledge of the energy calibration parameters (see
    e.g.~\cref{fig:gerda:calib-desc}), this contribution is negligible. The uncertainty on
    the other quantities appearing in the conversion factor between the reconstructed
    number of counts and the process half-life $\mathcal{C}_{76}$ in
    \cref{eq:2nbb-ana:halflife} is also negligible.

\end{description}

The effect of the inclusion of systematic uncertainties in the test-statistic sampling
distribution is shown in \cref{fig:2nbb-ana:ts-dist-sys}. Compared to
\cref{fig:2nbb-ana:ts-dist}, broader tails are observed at higher values of the test
statistic, resulting in larger confidence intervals on the signal strength. \pvalue\
curves, shown in the right-hand side of \cref{fig:2nbb-ana:pvalues}, are consequently
shifted to higher values of the assumed signal strength $S$. The effect of each source of
systematic uncertainty on the \nnbb\ half-life estimate and \onbbx\ ($n=1$) 90\%
C.L.~lower limit is reported in \cref{tab:2nbb-ana:systematics}.

\begin{figure}
  \centering
  \includegraphics{plots/2nbb-ana/2nbb-teststat-sys.pdf}
  \caption{%
    Sampling distribution of the test statistic probability $P$ for various signal
    hypothesis sampled with Monte Carlo methods, including the effect of systematic
    uncertainties. On the left: the standard \nnbb\ case. A signal of $S \sim 4.5 \cdot
    10^5$ counts in the analysis range is assumed in the toy experiments. On the right:
    sampling distributions for the null hypothesis on new-physics signals.  Deviations
    from the Wilks theorem predictions are observed. Bayesian blocks have been used to
    represent the histograms (see \cref{apdx:bayesblocks}). \fillme{update, add LV}
  }\label{fig:2nbb-ana:ts-dist-sys}
\end{figure}

\begin{table}
  \centering
  \caption{%
    Summary of the systematic uncertainties affecting the \nnbb\ distribution analysis.
    For brevity, only results for the ordinary \nnbb\ half-life estimate and the \onbbx\
    ($n=1$) 90\% C.L.~lower limit are reported.
    \fillme{numbers}
  }\label{tab:2nbb-ana:systematics}
  \input{tab/2nbb-ana/systematics.tex}
\end{table}

\section{Results and discussion}%
\label{sec:2nbb-ana:results}

Once the analysis procedure has been fixed and the distribution of the test statistic is
determined, its value is computed on physics data to extract the \nnbb\ half-life and 90\%
C.L.~lower limits for new-physics processes.

\blocktitle{\nnbb}
The estimate of the standard \nnbb-decay half-life is
\[
  \thalftwoM = (\fillme{?} \pm \stat{\fillme{?}} \pm \syst{\fillme{?}}) \cdot 10^{21}~\text{yr} \;.
\]
The statistical and systematic uncertainty is remarkably reduced compared to the estimate
extracted from \phaseone\ data before analysis cuts~\cite{Agostini2015a}. The improvement
in statistical uncertainty does not only arise from the higher data set exposure,
which is roughly doubled with respect to \phaseone, but also from the higher
signal-to-background ratio --- more than a factor 10 better. The level of systematic
uncertainty also benefits from the substantial cut of background events obtained with the
application of the LAr veto. As shown in \cref{sec:2nbb-ana:systematics}, the effect of
most systematic uncertainty sources is found in the shape of background \pdf{}s.
Uncertainties on the signal shape, from e.g.~nuclear theory or from the model of the detector dead
layer, are found to be sub-leading. The dominant contribution is the uncertainty on the
size of the detectors active volume. With a total uncertainty of 2.1\% on the \nnbb\
half-life, the measurement presented in this work is one of the most precise ever, on the
same footing of results from \textsc{CUPID-Mo} (2.9\%)~\cite{Armengaud2019}, CUPID-0
(2.2\%)~\cite{Azzolini2019a}, CUORE (2.8\%)~\cite{Caminata2019}, EXO-200
(2.8\%)~\cite{Albert2013} and \kamlandzen\ (3.4\%)~\cite{Gando2019} (see also
\cref{tab:nbb:2nbb-estimates}).

\blocktitle{new \\ physics}
Lower limits (90\% C.L.) on the half-lives of various Majoron-emitting modes of \onbb\ are
reported in \cref{tab:2nbb-ana:onbbx:limits}. Using phase space factors
from~\cite{Kotila2015} and the most recent set of nuclear matrix elements (the same as the
standard \onbb-decay for spectral index $n=1$,~\cite{Hirsch1995} for all the others) it is
possible to convert the half-life limit into a lower limit on the coupling constant \ga.
\newpar
The obtained 90\% C.L. confidence interval on the ratio between the number of counts in
the analysis range from the Lorentz- (and CPT-) violating double-beta decay mode \nnbblv\
and the standard \nnbb\ mode is:
\[
  \frac{S_\nnbblvM}{S_\nnbbM} \in [0,0] \;,
\]
The corresponding confidence interval on the \aof\ coefficient is calculated using phase
space factors in~\cite{Nitescu2020}:
\[
  \aofM \in [-0, 0] \cdot 10^{-5}~\text{GeV} \;.
\]

\fillme{more comments, compare with previous estimates and competing experiments}

\begin{table}
  \centering
  \caption{%
    90\% C.L.~lower limits for Majoron-emitting \onbb\ modes contributing to the \nnbb\
    event distribution. Nuclear matrix elements for spectral index $n=1$ are the same as
    the standard \onbb, and have been therefore selected from the most recent nuclear
    calculations.  Matrix elements for the other decay modes have been taken
    from~\cite{Hirsch1995}.  Phase space factors have been taken from~\cite{Kotila2015}.
    \fillme{should I use PSFs from klapdor to be consistent?}
    \fillme{numbers}
  }\label{tab:2nbb-ana:onbbx:limits}
  \input{tab/2nbb-ana/limits-onbbx.tex}
\end{table}

\begin{figure}
  \centering
  \includegraphics{plots/2nbb-ana/enrBEGe-results.pdf}
  \caption{%
    Best-fit model and data for the \enrBEGeII\ data set. The fit model does not include
    new-physics contributions, but the 90\% C.L.~upper limits on the
    hypothesized signals are reported. In the bottom panel the residuals are shown,
    together with normalized 68\%, 95\% and 99\% C.L.~intervals.
  }\label{fig:2nbb-ana:limits}
\end{figure}

\chapsummary
\begin{itemize}
  \item The development of a Monte Carlo model of the \gerda\ LAr veto system (described
    in \cref{chap:bkg:lar:ph2}) makes it possible to obtain predictions on the
    distribution of background and \nnbb\ events in the energy spectrum of data after the
    LAr veto cut. The high statistics of the \nnbb\ data sample ($\sim$$5 \cdot 10^4$
    events in \bege\ detectors above the \Arl\ Q-value from the first part of \phasetwo)
    and the excellent signal-to-background ratio after the LAr veto cut ($\sim$20, a
    factor $\sim$10 better than before the cut) motivates a high-precision analysis of its
    distribution to extract the \nnbb\ half-life and constrain the existence of
    new-physics phenomena (see \cref{chap:theory}).
  \item The data from \bege\ detectors from the first \gexpophasetwobkg\ of \phasetwo\
    is analyzed with a hybrid Bayesian-frequentist approach. Data from semi-coaxial
    detectors is discarded due to the large active volume uncertainties. A test statistic
    based on the profile likelihood ratio is defined, and its distribution with respect to
    the considered (new-)physics signal is studied with Monte Carlo methods. The analysis
    is optimized in order to maximize the sensitivity for limit setting to new-physics
    signals. Sensitivities to Majoron-emitting \onbb\ modes or Lorentz-violating \nnbb\
    are obtained in the $10^{23}$--$10^{24}$~yr.
  \item Various sources of systematic uncertainties are considered in the computation of
    the test statistic distribution by introducing distortions of the reference model of
    the Monte Carlo toy data sets. Various alternative LAr veto models and detector
    transition layer models are used to produce the background and signal \pdf{}s, but their
    effect on the test statistic distribution is of the order of 1\% or less. The main
    source of systematic uncertainty remains the detector active volume estimation, which
    contributes with \fillme{?}\% to the total uncertainty count.
  \item Final results for \thalftwo\ \fillme{\ldots}
\end{itemize}

% vim: tw=90
